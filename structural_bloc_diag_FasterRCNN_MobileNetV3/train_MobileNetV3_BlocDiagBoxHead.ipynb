{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bc4de-6005-41ba-8d9c-9e58bafbdf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models.detection import fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5afb63-4a1a-46a3-9ea9-76a1a497699d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bloc_diag_model.BlocDiagBoxHead import BlocDiagBoxHead\n",
    "from utils.train_utils import train_one_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea85a82a-2a00-4a4f-8b15-a097029198ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a1aa4b-dccc-445f-98e7-17a25ac97f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights and model\n",
    "weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(weights=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393f49c7-80b5-454b-a6ef-e6f01a395cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = model.roi_heads.box_head.fc6.in_features\n",
    "num_classes = 91\n",
    "\n",
    "\n",
    "original_box_head = model.roi_heads.box_head\n",
    "\n",
    "custom_box_head = BlocDiagBoxHead([12544, 1024, 1024], [[64]*16, [1024]], [[784]*16, [1024]], True)\n",
    "\n",
    "custom_box_head.fc6.load_state_dict(original_box_head.fc6.state_dict())\n",
    "custom_box_head.fc7.load_state_dict(original_box_head.fc7.state_dict())\n",
    "\n",
    "model.roi_heads.box_head = custom_box_head.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e17ddd-c27f-42bb-aacf-cc27db1443f6",
   "metadata": {},
   "source": [
    "##### sum of the L1 penalties of all off-block-diagonal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248c0ee-4b78-4567-9949-e063cd459256",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.roi_heads.box_head.get_off_diag_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d76dcb-db1a-42f7-bbb1-3f9ddb4f8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489c514-eb8d-4a57-b8c5-6cfa5d243014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transform used during pretraining\n",
    "transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc65c9-6b41-4c42-8ade-6aecef0ca7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data from https://cocodataset.org/#download\n",
    "data_dir = \"data/coco\"\n",
    "train_img_folder = os.path.join(data_dir, \"train2017\")\n",
    "train_ann_file = os.path.join(data_dir, \"annotations/instances_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b6c14c-75f5-4d47-ad4b-e621b00d4c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20000\n",
    "subset_indices = list(range(N))\n",
    "dataset_raw = torch.utils.data.Subset(CocoDetection(train_img_folder, train_ann_file), subset_indices)\n",
    "dataset = []\n",
    "for i in range(N):\n",
    "    img, target = dataset_raw[i]\n",
    "    img_id = dataset_raw.dataset.ids[dataset_raw.indices[i]]\n",
    "    transformed_img = transform(img)\n",
    "    for t in target:\n",
    "        t[\"image_id\"] = img_id\n",
    "    dataset.append((transformed_img, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096367b6-7250-42e2-b299-3b1dd64ec5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638d8f1-3c4c-48d3-a676-4e8bb0d7c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True, collate_fn=coco_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabb0ce6-9941-473a-b017-88596cc97f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3a6e9e-531d-4d67-86ae-629ff863632d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_epoch = 3\n",
    "lambda_offdiag = 1e-4\n",
    "timestamp_start = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "\n",
    "\n",
    "for epoch_num in range(n_total_epoch):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_one_epoch(model, data_loader, optimizer, epoch_num=epoch_num+1, n_total_epoch=n_total_epoch,\n",
    "                    device=device,\n",
    "                    metrics_path=os.path.join('./saved_models',\n",
    "                                              f\"train_metrics__lambda_offdiag={lambda_offdiag}__n_total_epoch={n_total_epoch}__{timestamp_start}.csv\"),\n",
    "                    lambda_offdiag=lambda_offdiag)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Epoch {epoch_num+1}/{n_total_epoch} completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "\n",
    "model_save_path = os.path.join('./saved_models',\n",
    "                               f\"model_BlocDiagBoxHead__lambda_offdiag={lambda_offdiag}__n_total_epoch={n_total_epoch}__{timestamp_start}.pt\")\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3dde63-78aa-4f39-b64a-08de2ac69105",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db702e36-6fbc-4f3f-8a4f-856d6635d767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c7fc7b-32ab-455e-a2ec-25998e3e2b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39b2fd-8eb8-49b8-9e4e-a64e029c59db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f81c571-84c4-4f6f-a15a-c6bf35fbca82",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd67783-9c43-4249-bba6-4663f560a747",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441305f0-46e2-4eac-ba7a-35dd22dd31c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_epoch = 3\n",
    "lambda_offdiag = 1e-4\n",
    "model_timestamp_start = '2025-04-28_14-16-04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97fadd0-80ff-4723-9ab8-af174f502470",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = os.path.join('./saved_models',\n",
    "                               f\"model_BlocDiagBoxHead__lambda_offdiag={lambda_offdiag}__n_total_epoch={n_total_epoch}__{model_timestamp_start}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d81ce2f-7d1e-4c02-945b-fc360c548d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_save_path))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f89469-cb4f-44cc-a3c7-d2ff7cbc6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.eval_utils import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c731cbc-57f9-452b-bb6c-281bd9c55ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the transform used during pretraining\n",
    "transform = weights.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1de5a1-96cd-4b59-b741-d919b37d7053",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"data/coco\"\n",
    "val_img_folder = os.path.join(data_dir, \"val2017\")\n",
    "val_ann_file = os.path.join(data_dir, \"annotations/instances_val2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62bec34-8052-4453-b149-091b601e7f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coco_collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5f0bef-fb2d-42fd-82c6-aec3f862206f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and apply transform manually to images only\n",
    "dataset_raw = CocoDetection(val_img_folder, val_ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7036502-de2d-4dff-9d93-0e1314017579",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in range(len(dataset_raw)):\n",
    "    img, target = dataset_raw[i]\n",
    "    img_id = dataset_raw.ids[i]\n",
    "    transformed_img = transform(img)\n",
    "    for t in target:\n",
    "        t[\"image_id\"] = img_id\n",
    "    dataset.append((transformed_img, target))\n",
    "\n",
    "val_data_loader = DataLoader(dataset, batch_size=1, shuffle=False, collate_fn=coco_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2566e93e-6610-4b89-be33-5954e884446f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate using pycocotools\n",
    "coco_gt = dataset_raw.coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662408e1-b42c-4277-aa81-1a0341cc19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file_path = os.path.join('./model_eval_results', f\"coco_val_results__{model_timestamp_start}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33438cfd-7667-40a6-8530-837bed2bc8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval model and save results to json\n",
    "evaluate_model(model, val_data_loader, device=device,\n",
    "               output_path=eval_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05578f-ecf0-43ef-b292-6a546e772bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_file_path)\n",
    "coco_dt = coco_gt.loadRes(eval_file_path)\n",
    "eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "eval.evaluate()\n",
    "eval.accumulate()\n",
    "eval.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902e0e82-ff97-440b-8ad2-dee206192cea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b173817-a6c2-4b69-8ee5-b5c4e36e6147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6a58a-59bd-4e84-9a45-9781ad8e0348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238ef7e7-6a0a-423d-84fe-b48b2b5c98f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d97a2-5e49-4451-8ef5-812d6a41c85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc6_layer = model.roi_heads.box_head.fc6\n",
    "\n",
    "# Get the weight matrix (shape: [1024, 12544])\n",
    "fc6_weight = fc6_layer.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Plot as a heatmap\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(fc6_weight, aspect='auto', interpolation='nearest', cmap='viridis', vmin=-0.01, vmax=0.01)\n",
    "plt.colorbar(label=\"Weight Value\")\n",
    "plt.title(\"Weight Matrix\")\n",
    "plt.xlabel(\"Input Features\")\n",
    "plt.ylabel(\"Output Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530bf4f7-4347-463f-81d6-06b26cc0b454",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (LSONN_MobileViT)",
   "language": "python",
   "name": "lsonn_mobilevit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
